import streamlit as st
import pandas as pd
import logica #Archivo con la l√≥gica de EvalIA
import ast

# --- CONFIGURACI√ìN DE P√ÅGINA ---
st.set_page_config(page_title="EvalIA - App", layout="wide")

# --- CSS ---
st.markdown("""
<style>
    .stTextArea textarea { font-size: 16px; }
    .stButton button { width: 100%; }
</style>
""", unsafe_allow_html=True)

# --- INICIALIZAR ESTADO ---
# Necesario para recordar la pregunta actual aunque recargues
if 'current_qid' not in st.session_state:
    st.session_state['current_qid'] = None
if 'df_preguntas' not in st.session_state:
    st.session_state['df_preguntas'] = logica.cargar_dataset()

# --- HEADER ---
st.title("üéì EvalIA: Sistema de Evaluaci√≥n Continua")
st.markdown("Capstone Project | Evaluaci√≥n autom√°tica de respuestas abiertas con **SBERT + Regresi√≥n Log√≠stica + GenAI**.")

# --- BARRA LATERAL (SIDEBAR) ---
with st.sidebar:
    st.header("Control")
    if st.button("üîÑ Cargar Nueva Pregunta"):
        if not st.session_state['df_preguntas'].empty:
            sample = st.session_state['df_preguntas'].sample(1).iloc[0]
            st.session_state['current_qid'] = sample['QUESTION_ID']
            # Limpiamos resultados anteriores
            if 'last_result' in st.session_state: del st.session_state['last_result']
            st.rerun()
        else:
            st.error("Error: Dataset no cargado.")
    
    st.divider()
    st.info("Este sistema asiste al profesor filtrando respuestas claras y marcando dudosas.")

# --- CUERPO PRINCIPAL ---

# 1. Mostrar Pregunta
if st.session_state['current_qid']:
    # Buscar datos de la pregunta actual
    df = st.session_state['df_preguntas']
    fila = df[df['QUESTION_ID'] == st.session_state['current_qid']].iloc[0]
    
    st.subheader(f"Pregunta {fila['QUESTION_ID']}")
    st.markdown(f"### {fila['QUESTION']}")
    
    # 2. Formulario de Respuesta
    with st.form("eval_form"):
        respuesta_usuario = st.text_area("Tu respuesta:", height=150, placeholder="Escribe aqu√≠ tu explicaci√≥n t√©cnica...")
        submitted = st.form_submit_button("üìä Evaluar Respuesta")
        
    if submitted:
        if not respuesta_usuario.strip():
            st.warning("Por favor, escribe algo antes de evaluar.")
        else:
            with st.spinner("Analizando sem√°ntica y generando feedback..."):
                # --- LLAMADA A LA L√ìGICA ---
                
                # A) C√°lculos Sem√°nticos
                resultado_metricas = logica.get_semantic_similarity(
                    model_correct=fila["ANSWER_CORRECT"],
                    model_wrong=fila["WRONG_EXAMPLES"],
                    student_answer=respuesta_usuario,
                    keywords=logica.parse_list(fila.get("KEYWORDS", []))
                )
                
                # B) Clasificaci√≥n (Regresi√≥n Log√≠stica)
                score = logica.scorer_logreg_kw(resultado_metricas)
                interpretacion = logica.interpretar_3clases(score)
                
                # C) Feedback Generativo (Gemini)
                feedback_ia, modelo = logica.generar_feedback_genai(
                    pregunta=fila["QUESTION"],
                    student_answer=respuesta_usuario,
                    interpretacion=interpretacion,
                    referencia=fila["ANSWER_CORRECT"],
                    hint=fila["HINT"]
                )
                
                # Guardamos resultados en sesi√≥n para que no desaparezcan
                st.session_state['last_result'] = {
                    "interpretacion": interpretacion,
                    "feedback": feedback_ia,
                    "score": score,
                    "metrics": resultado_metricas,
                    "referencia": fila["ANSWER_CORRECT"],
                    "hint": fila["HINT"]
                }
                st.rerun()

# 3. Mostrar Resultados
if 'last_result' in st.session_state:
    res = st.session_state['last_result']
    
    st.divider()
    
    # Encabezado de resultado con color
    if res['interpretacion'] == 'Correcta':
        st.success(f"‚úÖ Resultado: **CORRECTA** (Confianza: {res['score']:.2f})")
    elif res['interpretacion'] == 'Incorrecta':
        st.error(f"‚ùå Resultado: **INCORRECTA** (Confianza: {res['score']:.2f})")
    else:
        st.warning(f"ü§î Resultado: **REVISI√ìN NECESARIA** (Confianza: {res['score']:.2f})")
    
    # Columnas para Feedback y M√©tricas
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("### ü§ñ Feedback IA")
        st.info(res['feedback'])
        
        with st.expander("Ver respuesta de referencia (Profesor)"):
            try:
                refs = ast.literal_eval(res['referencia']) if isinstance(res['referencia'], str) else res['referencia']
                st.write(refs[0] if isinstance(refs, list) and len(refs)>0 else str(refs))
            except:
                st.write(res['referencia'])
                
    with col2:
        st.markdown("### üìà M√©tricas T√©cnicas")
        st.metric("Similitud con respuestas CORRECTAS", f"{res['metrics']['max_correct']:.2f}")
        st.metric("Similitud con respuestas INCORRECTAS", f"{res['metrics']['max_wrong']:.2f}")
        st.metric("Uso de Keywords (F1)", f"{res['metrics']['kw_f1']:.2f}")

else:
    if not st.session_state['current_qid']:
        st.info("üëà Pulsa 'Cargar Nueva Pregunta' en la barra lateral para comenzar.")

        
