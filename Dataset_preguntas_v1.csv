QUESTION_ID,MODULE,TOPIC,QUESTION,HINT,ANSWER_CORRECT,WRONG_EXAMPLES,KEYWORDS
Q001,NLP,Generación de texto con RNN,¿Cuál es el propósito principal de la codificación one-hot al preparar datos de texto para una RNN?,Considere qué tipo de datos puede entender y procesar una red neuronal directamente.,"[""Transformar palabras o caracteres en vectores numéricos que la red neuronal puede procesar."", ""Transformar token(s) o caracteres en vectores numéricos que la red neuronal puede procesar."", ""Convertir palabras o caracteres en vectores numéricos que la red neuronal puede procesar."", ""Transformar palabras o caracteres en representación(es) numérica(s) que la red neuronal puede procesar.""]","[""Medir la diferencia entre las predicciones del modelo y los valores reales."", ""Ajustar los pesos de la red para reducir el error durante el entrenamiento."", ""Introducir aleatoriedad en la selección de la siguiente palabra durante la generación de texto.""]","['neuronal', 'vectores', 'caracteres', 'convertir', 'palabras', 'transformar', 'procesar', 'numéricos', 'token', 'numérica']"
Q002,NLP,Generación de texto con RNN,"En el contexto del entrenamiento de una RNN, ¿cuál es la función de un optimizador como Adam o el Descenso de Gradiente Estocástico (SGD)?",Este componente es el responsable del proceso de 'aprendizaje' durante el entrenamiento.,"[""Ajustar los pesos de la red para reducir la función de pérdida."", ""Síntesis: Ajustar los pesos de la red para minimizar la función de pérdida."", ""Ajustar los pesos de la red para minimizar la función de pérdida.""]","[""Evaluar el rendimiento final del modelo en un conjunto de datos de validación."", ""Definir la arquitectura de la red, incluyendo el número de capas y neuronas."", ""Convertir el texto de entrada en secuencias de vectores numéricos.""]","['minimizar', 'reducir', 'pérdida', 'pesos', 'ajustar', 'función', 'red']"
Q003,NLP,Generación de texto con RNN,"Al generar texto con una RNN entrenada, ¿qué es una 'semilla' (seed)?",Conecta el concepto con su uso práctico en el flujo del módulo.,"[""Una palabra o secuencia de palabras inicial que se proporciona al modelo para comenzar la generación."", ""Síntesis: Una palabra o secuencia de palabras inicial que se proporciona al modelo"", ""Una token(s) o secuencia de token(s) inicial que se proporciona al modelo para comenzar la generación.""]","[""El conjunto completo de datos utilizado para entrenar el modelo de lenguaje."", ""Un hiperparámetro que controla la creatividad o aleatoriedad del texto generado."", ""La función que mide el error del modelo durante la fase de entrenamiento.""]","['secuencia', 'modelo', 'palabra', 'generación', 'palabras', 'inicial', 'comenzar', 'token', 'proporciona']"
Q004,NLP,Generación de texto con RNN,¿Por qué las variantes de RNN como LSTM y GRU son a menudo preferidas sobre las RNN simples para tareas de texto?,El texto menciona un desafío relacionado con el contexto y la memoria en secuencias extensas.,"[""Síntesis: Porque son más efectivas para modelar dependencias a largo plazo en los"", ""Porque son más efectivas para modelar dependencias a largo plazo en los datos."", ""Porque son más efectivas para modelar dependencias a largo plazo en los datos. (equivalente)""]","[""Porque garantizan la generación de texto sin errores gramaticales."", ""Porque requieren significativamente menos datos para su entrenamiento."", ""Porque utilizan la codificación one-hot de manera más eficiente.""]","['modelar', 'datos', 'efectivas', 'dependencias', 'plazo', 'largo']"
Q005,NLP,Generación de texto con RNN,¿Da ejemplos de aplicación de la generación de texto con RNN?,Busque una tarea que implique crear una nueva secuencia de palabras a partir de una secuencia existente.,"[""Traducción automática de un idioma a otro."", ""Síntesis: Traducción automática de un idioma a otro."", ""Traducción automática de un idioma a otro. (equivalente)""]","[""Predicción de precios en el mercado de valores."", ""Compresión de archivos de datos."", ""Clasificación de imágenes.""]","['traducción', 'idioma', 'automática']"
Q006,NLP,Generación de texto con RNN,¿Qué implica que el proceso de generación de texto con una RNN sea 'estocástico'?,Esta propiedad es la que permite que los resultados sean más creativos y menos predecibles.,"[""Síntesis: Que el modelo puede producir diferentes textos incluso con la misma entrada"", ""Que el modelo puede producir diferentes textos incluso con la misma entrada inicial."", ""Que el modelo puede producir diferentes textos incluso con la misma entrada inicial. (equivalente)""]","[""Que el modelo siempre elige la única palabra siguiente más probable."", ""Que el modelo solo puede ser entrenado una única vez con un conjunto de datos."", ""Que el modelo requiere un número fijo de palabras para generar una secuencia completa.""]","['modelo', 'textos', 'diferentes', 'inicial', 'puede', 'incluso', 'entrada', 'producir', 'misma']"
Q007,NLP,Generación de texto con RNN,¿Qué función de pérdida se utiliza comúnmente para las tareas de generación de texto?,Esta función matemática mide qué tan diferente es la distribución de probabilidad predicha por el modelo de la distribución real.,"[""Entropía cruzada."", ""Síntesis: Entropía cruzada."", ""Entropía cruzada. (equivalente)""]","[""Descenso de Gradiente Estocástico (SGD)."", ""Codificación one-hot."", ""Temperatura de muestreo.""]","['entropía', 'cruzada']"
Q008,NLP,Generación de texto con RNN,Las RNN han sido superadas por arquitecturas más avanzadas en muchas tareas. ¿Qué arquitecturas son alternativas superiores?,Estas arquitecturas se mencionan en la introducción como sucesoras de las RNN.,"[""CNN y Transformaciones de atención."", ""Síntesis: CNN y Transformaciones de atención."", ""CNN y Transformaciones de atención. (equivalente)""]","[""Máquinas de Vectores de Soporte (SVM) y K-Means."", ""Perceptrones Multicapa (MLP) y Árboles de Decisión."", ""LSTM y GRU.""]","['cnn', 'transformaciones', 'atención']"
Q009,NLP,Generación de texto con RNN,¿Cuál es el objetivo principal de la fase de entrenamiento de una RNN para generación de texto?,El propósito de entrenar un modelo es acercar sus resultados a los datos correctos.,"[""Síntesis: Minimizar la diferencia entre las predicciones del modelo y los siguientes elementos"", ""Minimizar la diferencia entre las predicciones del modelo y los siguientes elementos reales en las secuencias de entrenamiento."", ""Minimizar la diferencia entre las predicciones del modelo y los siguientes elementos reales en las secuencias de entrenamiento. (equivalente)""]","[""Definir el número de capas ocultas y neuronas de la red."", ""Generar texto de la manera más rápida posible, sin importar la coherencia."", ""Crear el vocabulario más grande posible a partir del texto de entrada.""]","['modelo', 'entrenamiento', 'minimizar', 'predicciones', 'siguientes', 'secuencias', 'diferencia', 'reales', 'elementos']"
Q010,NLP,Generación de texto con RNN,¿Cómo influye la 'temperatura de muestreo' en el texto generado por una RNN?,Piense en este parámetro como una perilla que ajusta el nivel de creatividad.,"[""Una temperatura más alta aumenta la diversidad y aleatoriedad en la selección de token(s)."", ""Una temperatura más alta aumenta la diversidad y aleatoriedad en la selección de palabras."", ""Síntesis: Una temperatura más alta aumenta la diversidad y aleatoriedad en la selección""]","[""Una temperatura más alta hace que el modelo elija solo las palabras más probables."", ""Se utiliza únicamente durante la fase de entrenamiento para ajustar la tasa de aprendizaje."", ""Controla directamente la longitud final del texto que se genera.""]","['aleatoriedad', 'diversidad', 'selección', 'aumenta', 'temperatura', 'palabras', 'alta', 'token']"
Q011,NLP,Clasificación de opiniones / Análisis de sentimiento (RNN),"Según el texto, ¿cuál es el propósito principal del 'Análisis de polaridad' en el contexto del análisis de sentimientos?",Considera la forma más fundamental de categorizar una opinión.,"[""Clasificar el sentimiento general de un texto como positivo, negativo o neutral."", ""Síntesis: Clasificar el sentimiento general de un texto como positivo, negativo o neutral."", ""Clasificar el sentimiento general de un texto como positivo, negativo o neutral. (equivalente)""]","[""Agrupar documentos por temas comunes."", ""Detectar sarcasmo y dobles sentidos en el lenguaje."", ""Identificar emociones específicas como 'feliz' o 'triste'.""]","['negativo', 'sentimiento', 'neutral', 'positivo', 'clasificar', 'texto', 'general']"
Q012,NLP,Conjunto de datos y preprocesamiento (RNN/NLP),¿Cuál de los siguientes pasos del proceso de análisis de sentimientos implica dividir el texto en palabras o 'tokens' individuales?,Piensa en cómo una máquina comenzaría a 'leer' una oración.,"[""Tokenización."", ""Síntesis: Tokenización."", ""Tokenización. (equivalente)""]","[""Recopilación de datos"", ""Interpretación"", ""Preprocesamiento de los datos""]",['tokenización']
Q013,NLP,Generación de texto con RNN,"En el contexto de la clasificación de opiniones con RNN, ¿para qué se utiliza un conjunto de datos de prueba?","Una vez que el modelo ha sido entrenado, ¿cómo se comprueba su eficacia real?","[""Para medir qué tan bien el modelo generaliza a textos que no ha visto antes."", ""Síntesis: Para medir qué tan bien el modelo generaliza a textos que no"", ""Para medir qué tan bien el modelo generaliza a textos que no ha visto antes. (equivalente)""]","[""Para ajustar los pesos del modelo durante el descenso de gradiente."", ""Para convertir las palabras del texto en números o tokens."", ""Para alimentar a la RNN con nuevos datos y seguir aprendiendo.""]","['modelo', 'textos', 'generaliza', 'visto', 'medir', 'bien', 'tan']"
Q014,NLP,Clasificación de opiniones / Análisis de sentimiento (RNN),¿Qué tipo de arquitectura de red neuronal se usa para medir similitud entre dos datos?,Revisa la lista de técnicas específicas para la detección de temas.,"[""Redes Neuronales Siamesas"", ""Síntesis: Redes Neuronales Siamesas"", ""Redes Neuronales Siamesas (equivalente)""]","[""Análisis semántico latente (LSA)"", ""Latent Dirichlet Allocation (LDA)"", ""Factorización de matrices no negativas (NMF)""]","['neuronales', 'redes', 'siamesas']"
Q015,NLP,Generación de texto con RNN,¿Por qué las Redes Neuronales Recurrentes (RNN) son especialmente adecuadas para el análisis de sentimientos?,Piensa en la naturaleza secuencial del lenguaje y el procesamiento palabra por palabra.,"[""Porque pueden manejar secuencias de texto y mantener una 'memoria' de lo que han visto previamente."", ""Síntesis: Porque pueden manejar secuencias de texto y mantener una 'memoria' de lo"", ""Porque pueden manejar secuencias de texto y mantener una 'memoria' de lo que han visto previamente. (equivalente)""]","[""Porque identifican frases y oraciones de sentimiento local en un texto."", ""Porque aprenden representaciones eficientes y comprimidas de los datos de entrada."", ""Porque utilizan mecanismos de atención para mejorar la velocidad del entrenamiento.""]","['memoria', 'secuencias', 'texto', 'mantener', 'pueden', 'visto', 'previamente', 'manejar']"
Q016,NLP,Mecanismos de atención,Los modelos basados en Transformers como BERT y GPT se han convertido en el estado del arte para muchas tareas de NLP. ¿Qué mecanismo clave utilizan estas arquitecturas?,Esta arquitectura no procesa secuencialmente; pondera la importancia de distintas palabras.,"[""Síntesis: Mecanismos de atención"", ""Mecanismos de atención"", ""Mecanismos de atención (equivalente)""]","[""Unidades recurrentes cerradas (GRU)"", ""Descomposición en valores singulares (SVD)"", ""Capas convolucionales""]","['mecanismos', 'atención']"
Q017,NLP,Clasificación de opiniones / Análisis de sentimiento (RNN),"¿Cuáles son los desafíos del análisis de sentimientos destacado, relacionado con la naturaleza subjetiva del lenguaje humano?",Piensa en frases cuyo significado es el opuesto a las palabras literales.,"[""La interpretación del sarcasmo, el doble sentido y la jerga."", ""Síntesis: La interpretación del sarcasmo, el doble sentido y la jerga."", ""La interpretación del sarcasmo, el doble sentido y la jerga. (equivalente)""]","[""La selección de la arquitectura de red neuronal adecuada."", ""La dificultad de eliminar palabras vacías y signos de puntuación."", ""La necesidad de grandes volúmenes de datos para el entrenamiento.""]","['sarcasmo', 'interpretación', 'doble', 'jerga']"
Q018,NLP,Clasificación de opiniones / Análisis de sentimiento (RNN),¿Qué técnica de modelado de temas busca descomponer una matriz de término-documento en matrices más pequeñas que representan temas y sus asociaciones con los documentos?,El nombre de la técnica describe la operación matemática que realiza.,"[""Síntesis: Factorización de matrices no negativas (NMF)"", ""Factorización de matrices no negativas (NMF)"", ""Factorización de matrices no negativas (NMF) (equivalente)""]","[""Redes Neuronales Convolucionales (CNN)"", ""Redes neuronales de transformadores"", ""Análisis de polaridad""]","['factorización', 'matrices', 'nmf', 'negativas']"
Q019,NLP,Clasificación de opiniones / Análisis de sentimiento (RNN),¿Cómo se pueden utilizar el modelado de temas y el análisis de sentimientos de forma conjunta?,Proceso de dos pasos: ¿de qué se habla? y ¿cómo se sienten?,"[""Síntesis: Identificando temas y luego analizando el sentimiento de los comentarios para cada"", ""Identificando temas y luego analizando el sentimiento de los comentarios para cada tema específico."", ""Identificando temas y luego analizando el sentimiento de los comentarios para cada tema específico. (equivalente)""]","[""Entrenando un único modelo que realice ambas tareas simultáneamente sin diferenciarlas."", ""Reemplazando el análisis de sentimientos con el modelado de temas para mayor precisión."", ""Utilizando el modelado de temas para preprocesar los datos antes de aplicar el análisis de sentimientos.""]","['analizando', 'comentarios', 'identificando', 'sentimiento', 'tema', 'específico', 'cada', 'temas', 'luego']"
Q020,NLP,Clasificación de opiniones / Análisis de sentimiento (RNN),¿Qué tipo de arquitectura de aprendizaje profundo se menciona como útil para comparar un texto con ejemplos conocidos de texto positivo y negativo para determinar la similitud?,Arquitectura que procesa dos entradas en paralelo para evaluar su relación.,"[""Redes Neuronales Siamesas"", ""Síntesis: Redes Neuronales Siamesas"", ""Redes Neuronales Siamesas (equivalente)""]","[""Autoencoders"", ""Redes Neuronales Convolucionales (CNN)"", ""Redes Neuronales Recurrentes (RNN)""]","['neuronales', 'redes', 'siamesas']"
Q021,NLP,Conjunto de datos y preprocesamiento (RNN/NLP),¿Cuál es el propósito principal de dividir el conjunto de datos en un subconjunto de entrenamiento y otro de validación?,Sirve para detectar sobreajuste y evaluar rendimiento generalizable.,"[""Evaluar el rendimiento del modelo y ayudar a prevenir el sobreajuste."", ""Síntesis: Evaluar el rendimiento del modelo y ayudar a prevenir el sobreajuste."", ""Evaluar el rendimiento del modelo y ayudar a prevenir el sobreajuste. (equivalente)""]","[""Acelerar el proceso de recolección de datos inicial."", ""Limpiar y normalizar el texto de manera más eficiente."", ""Aumentar la cantidad total de datos disponibles para el entrenamiento.""]","['modelo', 'evaluar', 'rendimiento', 'sobreajuste', 'prevenir', 'ayudar']"
Q022,NLP,Generación de texto con RNN,"En el contexto de la preparación de datos para una RNN, ¿qué es la tokenización?",Cómo una computadora 'lee' una oración para procesarla.,"[""El proceso de dividir el texto en unidades más pequeñas, como palabras o caracteres."", ""El proceso de dividir el texto en unidades más pequeñas, como token(s) o caracteres."", ""Síntesis: El proceso de dividir el texto en unidades más pequeñas, como palabras""]","[""La transformación de texto en vectores numéricos usando codificación one-hot."", ""El proceso de convertir todos los caracteres del texto a minúsculas."", ""La eliminación de palabras y puntuación no deseadas del corpus.""]","['dividir', 'texto', 'caracteres', 'palabras', 'pequeñas', 'unidades', 'proceso', 'token']"
Q023,NLP,Conjunto de datos y preprocesamiento (RNN/NLP),"Si un conjunto de datos es demasiado grande para caber en la memoria RAM, ¿cuál de las siguientes es una solución recomendada?","Si el dataset no cabe en memoria, piensa en procesarlo por partes sin cargarlo todo a la vez.","[""Síntesis: Usar Data Loaders o Generators para cargar y preprocesar los datos en"", ""Usar Data Loaders o Generators para cargar y preprocesar los datos en lotes."", ""Usar Data Loaders o Generators para cargar y preprocesar los datos en lotes. (equivalente)""]","[""Comprimir todo el conjunto de datos en un único archivo zip para reducir su tamaño."", ""Aumentar la cantidad de signos de puntuación para delimitar mejor los datos."", ""Dividir el conjunto de datos en un 99% para validación y 1% para entrenamiento.""]","['loaders', 'generators', 'preprocesar', 'data', 'datos', 'cargar', 'usar', 'lotes']"
Q024,NLP,Generación de texto con RNN,"Al crear secuencias de entrenamiento para una RNN de generación de texto, ¿qué representa el 'objetivo' (target)?",La red aprende a anticipar lo que viene inmediatamente después.,"[""Síntesis: El siguiente token en el texto que la RNN debe aprender a"", ""El siguiente token en el texto que la RNN debe aprender a predecir."", ""El siguiente token en el texto que la RNN debe aprender a predecir. (equivalente)""]","[""La secuencia completa de entrada codificada numéricamente."", ""Un valor numérico que indica la longitud de la secuencia."", ""El primer token de la secuencia de entrada.""]","['aprender', 'siguiente', 'rnn', 'token', 'texto', 'predecir', 'debe']"
Q025,NLP,Conjunto de datos y preprocesamiento (RNN/NLP),"¿Cuál es el propósito de la normalización de datos, como convertir todo el texto a minúsculas?",Busca coherencia en el formato del texto para evitar confusiones.,"[""Asegurar que palabras como 'Hola' y 'hola' se traten como la misma palabra."", ""Asegurar que token(s) como 'Hola' y 'hola' se traten como la misma token(s)."", ""Síntesis: Asegurar que palabras como 'Hola' y 'hola' se traten como la misma""]","[""Aumentar la variedad de ejemplos de texto en el conjunto de datos."", ""Dividir el texto en sus caracteres o palabras constituyentes."", ""Eliminar palabras muy comunes que no aportan significado.""]","['hola', 'palabras', 'palabra', 'token', 'asegurar', 'traten', 'misma']"
Q026,NLP,Generación de texto con RNN,¿Qué técnica de codificación es para transformar los tokens en un formato numérico que la RNN pueda procesar?,Representa cada palabra o carácter como un vector con muchos ceros.,"[""Codificación one hot."", ""Codificación one-hot."", ""Síntesis: Codificación one-hot.""]","[""Normalización de datos."", ""Codificación de secuencias."", ""Almacenamiento en la nube.""]","['codificación', 'one', 'hot']"
Q027,NLP,Conjunto de datos y preprocesamiento (RNN/NLP),"Dentro del proceso de 'Limpieza y transformación de datos', ¿qué paso implica la eliminación de datos no deseados como signos de puntuación innecesarios o caracteres especiales?",Es uno de los primeros pasos para eliminar ruido antes de procesar.,"[""Limpieza de datos."", ""Síntesis: Limpieza de datos."", ""Limpieza de datos. (equivalente)""]","[""Codificación."", ""Tokenización."", ""Creación de secuencias.""]","['limpieza', 'datos']"
Q028,NLP,Conjunto de datos y preprocesamiento (RNN/NLP),¿Cuáles son las etapas principales para preparar un conjunto de datos antes de entrenar un modelo de generación de texto?,Orden lógico: recolectar → limpiar/tokenizar → codificar → secuencias → split.,"[""Recolección, preprocesamiento (limpieza, tokenización, codificación), creación de secuencias y división de datos."", ""Síntesis: Recolección, preprocesamiento (limpieza, tokenización, codificación), creación de secuencias y división de datos."", ""Recolección, preprocesamiento (limpieza, tokenización, codificación), creación de secuencias y división de datos. (equivalente)""]","[""Codificación, creación de secuencias, validación y almacenamiento en la nube."", ""Conclusión basada en intuición sin datos"", ""Usar azar para seleccionar la salida""]","['preprocesamiento', 'codificación', 'secuencias', 'tokenización', 'recolección', 'datos', 'limpieza', 'división', 'creación']"
Q029,NLP,Generación de texto con RNN,¿Cuál es la principal limitación de las arquitecturas RNN tradicionales que los mecanismos de atención y los modelos Transformers buscan resolver?,Procesan paso a paso; la información inicial se puede perder en secuencias largas.,"[""Síntesis: La dificultad para capturar dependencias a largo plazo en secuencias extensas."", ""La dificultad para capturar dependencias a largo plazo en secuencias extensas."", ""La dificultad para capturar dependencias a largo plazo en secuencias extensas. (equivalente)""]","[""El alto costo computacional para procesar secuencias cortas."", ""La complejidad en el preprocesamiento de los datos de texto."", ""La incapacidad de procesar datos que no sean de texto.""]","['secuencias', 'dificultad', 'dependencias', 'extensas', 'capturar', 'plazo', 'largo']"
Q030,NLP,Conjunto de datos y preprocesamiento (RNN/NLP),"En el desarrollo de una aplicación de NLP, ¿qué es la tokenización?",Primer paso para que una máquina 'lea' una oración.,"[""Dividir el texto en unidades más pequeñas, como token(s) o caracteres."", ""Síntesis: Dividir el texto en unidades más pequeñas, como palabras o caracteres."", ""Dividir el texto en unidades más pequeñas, como palabras o caracteres.""]","[""Eliminar palabras comunes y sin significado (stop words) del texto."", ""Convertir los tokens de texto en vectores numéricos."", ""Separar el conjunto de datos en subconjuntos de entrenamiento, validación y prueba.""]","['dividir', 'caracteres', 'texto', 'unidades', 'pequeñas', 'palabras', 'token']"
Q031,NLP,Codificador–Decodificador y Traducción automática,"En una arquitectura de red encoder-decoder para traducción automática, ¿cuál es la función principal del vector de contexto?",El codificador debe 'comunicar' el significado de la oración al decodificador.,"[""Síntesis: Actuar como una representación numérica que resume la información semántica de la"", ""Actuar como una representación numérica que resume la información semántica de la secuencia de entrada."", ""Actuar como una representación numérica que resume la información semántica de la secuencia de entrada. (equivalente)""]","[""Almacenar la secuencia de salida traducida palabra por palabra."", ""Contener los hiperparámetros del modelo, como la tasa de aprendizaje."", ""Guardar los pesos de la red neuronal del decodificador.""]","['representación', 'secuencia', 'semántica', 'información', 'actuar', 'numérica', 'entrada', 'resume']"
Q032,NLP,Generación de texto con RNN,¿Cuál es la diferencia fundamental en cómo un modelo Transformer y una RNN procesan una secuencia de datos?,Piensa en recurrencia frente a paralelización.,"[""El Transformer procesa todos los elementos de la secuencia simultáneamente, mientras que la RNN lo hace de forma secuencial."", ""Síntesis: El Transformer procesa todos los elementos de la secuencia simultáneamente, mientras que"", ""El Transformer procesa todos los elementos de la secuencia simultáneamente, mientras que la RNN lo hace de forma secuencial. (equivalente)""]","[""El Transformer requiere datos etiquetados mientras que la RNN no."", ""El Transformer utiliza capas convolucionales mientras que la RNN no."", ""La RNN puede manejar secuencias de longitud variable, pero el Transformer no.""]","['transformer', 'secuencial', 'secuencia', 'simultáneamente', 'procesa', 'rnn', 'forma', 'elementos', 'hace']"
Q033,NLP,Conjunto de datos y preprocesamiento (RNN/NLP),"En un modelo Transformer, ¿cuál es el propósito de las codificaciones posicionales (positional encodings)?",Se pierde el orden si procesas todas las palabras a la vez.,"[""Síntesis: Añadir información sobre el orden o la posición de los tokens en"", ""Añadir información sobre el orden o la posición de los tokens en la secuencia."", ""Añadir información sobre el orden o la posición de los tokens en la secuencia. (equivalente)""]","[""Indicar el sentimiento general de la secuencia."", ""Comprimir la secuencia de entrada en un vector de tamaño fijo."", ""Reducir la dimensionalidad de los vectores de palabras (embeddings).""]","['tokens', 'secuencia', 'orden', 'posición', 'información', 'añadir']"
Q034,NLP,Modelos Transformers (NLP),¿Cómo adapta la arquitectura Vision Transformer (ViT) el modelo Transformer para el procesamiento de imágenes?,Convierte la imagen en una secuencia de parches (tokens).,"[""Dividiendo la imagen en una cuadrícula de parches y tratándolos como una secuencia de tokens."", ""Síntesis: Dividiendo la imagen en una cuadrícula de parches y tratándolos como una"", ""Dividiendo la imagen en una cuadrícula de parches y tratándolos como una secuencia de tokens. (equivalente)""]","[""Convirtiendo la imagen a un texto descriptivo y luego procesando ese texto."", ""Procesando cada píxel de la imagen de forma recurrente."", ""Aplicando múltiples capas de convolución para extraer características antes de usar la atención.""]","['cuadrícula', 'tokens', 'dividiendo', 'imagen', 'parches', 'secuencia', 'tratándolos']"
Q035,NLP,Modelos Transformers (NLP),¿Cuáles son las ventajas más destacadas de la librería de Transformers de Hugging Face?,Popular por acceso rápido a modelos preentrenados.,"[""Síntesis: Ofrece un acceso simplificado a una vasta colección de modelos preentrenados."", ""Ofrece un acceso simplificado a una vasta colección de modelos preentrenados."", ""Ofrece un acceso simplificado a una vasta colección de modelos preentrenados. (equivalente)""]","[""Funciona exclusivamente con el framework de aprendizaje profundo PyTorch."", ""Está diseñada únicamente para tareas de visión por computadora."", ""Es la única librería que permite entrenar modelos Transformer desde cero.""]","['modelos', 'simplificado', 'preentrenados', 'colección', 'acceso', 'ofrece', 'vasta']"
Q036,NLP,Codificador–Decodificador y Traducción automática,"Según la comparativa de librerías, ¿qué herramienta está principalmente enfocada en la traducción automática?",El nombre de la librería da una pista clara de su propósito.,"[""Síntesis: OpenNMT"", ""OpenNMT"", ""OpenNMT (equivalente)""]","[""AllenNLP"", ""Fairseq"", ""spaCy Transformers""]",['opennmt']
Q037,NLP,Clasificación de opiniones / Análisis de sentimiento (RNN),"En el contexto del análisis de sentimientos, ¿qué buscan descubrir técnicas de modelado de temas como Latent Dirichlet Allocation (LDA)?",Piensa en aspectos más comentados sin leer todas las reseñas.,"[""Síntesis: Los temas o tópicos abstractos presentes en un conjunto de documentos."", ""Los temas o tópicos abstractos presentes en un conjunto de documentos."", ""Los temas o tópicos abstractos presentes en un conjunto de documentos. (equivalente)""]","[""La traducción de un comentario a otro idioma."", ""La polaridad (positiva, negativa o neutral) de un comentario."", ""La estructura gramatical de las oraciones en los comentarios.""]","['tópicos', 'abstractos', 'documentos', 'conjunto', 'temas', 'presentes']"
Q038,NLP,Generación de texto con RNN,"Además de las RNN, ¿qué otra arquitectura de red neuronal se menciona como efectiva para el análisis de sentimientos y la identificación de frases locales?","En visión, detecta patrones locales como bordes o texturas.","[""Redes Neuronales Convolucionales (CNN)"", ""Síntesis: Redes Neuronales Convolucionales (CNN)"", ""Redes Neuronales Convolucionales (CNN) (equivalente)""]","[""Autoencoders"", ""Latent Dirichlet Allocation (LDA)"", ""Redes Neuronales Siamesas""]","['neuronales', 'cnn', 'redes', 'convolucionales']"
Q039,NLP,Mecanismos de atención,Bahdanau y Luong son dos implementaciones conocidas de ¿qué componente clave en los modelos de secuencia a secuencia?,Permite 'enfocarse' en partes específicas de la entrada.,"[""Síntesis: Mecanismos de atención"", ""Mecanismos de atención"", ""Mecanismos de atención (equivalente)""]","[""Funciones de activación"", ""Algoritmos de optimización"", ""Células de memoria RNN""]","['mecanismos', 'atención']"
Q040,NLP,Generación de texto con RNN,¿Qué ventaja tienen las redes Transformer en comparación con las RNN respecto a la cantidad de datos necesarios para entrenar?,Considera el apetito por los datos que tienen los grandes modelos de lenguaje modernos.,"[""Menor necesidad de datos de entrenamiento en comparación con las RNN."", ""Síntesis: Menor necesidad de datos de entrenamiento en comparación con las RNN."", ""Menor necesidad de datos de entrenamiento en comparación con las RNN. (equivalente)""]","[""Paralelización eficiente durante el entrenamiento."", ""Capacidad para capturar dependencias a largo plazo."", ""Escalabilidad para entrenar modelos masivos como GPT-3.""]","['entrenamiento', 'datos', 'rnn', 'comparación', 'menor', 'necesidad']"
Q041,NLP,Conjunto de datos y preprocesamiento (RNN/NLP),"Al almacenar un conjunto de datos de entrenamiento demasiado grande para caber en la memoria, ¿qué herramienta de las librerías de aprendizaje profundo carga y preprocesa los datos en lotes durante el entrenamiento?","Busca la opción que describe una función o clase diseñada para alimentar datos a un modelo de manera eficiente, lote por lote.","[""Síntesis: Data Loaders o Generators."", ""Data Loaders o Generators."", ""Data Loaders o Generators. (equivalente)""]","[""Un archivo de formato HDF5."", ""Un servicio de almacenamiento en la nube como AWS S3."", ""Una base de datos SQL.""]","['generators', 'loaders', 'data']"
Q042,M12T1,Generación de texto con RNN,"En generación de texto con RNN, ¿qué efecto tiene aumentar la 'temperatura' al muestrear el siguiente token?",Relaciona temperatura con diversidad vs. determinismo en la muestra.,"[""Aumenta la aleatoriedad y diversidad de las salidas"", ""Hace que el modelo elija opciones menos probables con mayor frecuencia"", ""Reduce el sesgo hacia el token más probable, generando textos más creativos""]","[""Disminuye la entropía y hace el texto más repetitivo"", ""Obliga a predecir siempre el token con mayor probabilidad"", ""Desactiva el estado oculto de la RNN para evitar sobreajuste""]","['aleatoriedad', 'probables', 'opciones', 'modelo', 'diversidad', 'creativos', 'probable', 'salidas', 'reduce', 'menos']"
Q043,M12T1,Generación de texto con RNN,Caso práctico: Tu modelo LSTM genera frases incoherentes a mitad de secuencia. ¿Qué ajuste sería más apropiado probar primero?,"Piensa en longitud de secuencia, regularización y tamaño de capa.","[""Aumentar el tamaño del estado oculto o la profundidad para modelar dependencias más largas"", ""Ajustar la longitud de las secuencias de entrenamiento (window)"", ""Aplicar regularización moderada (dropout) para mejorar la estabilidad""]","[""Eliminar la codificación numérica y entrenar con texto crudo"", ""Sustituir LSTM por una red completamente conectada sin memoria"", ""Desactivar el teacher forcing en una tarea puramente no supervisada""]","['regularización', 'entrenamiento', 'modelar', 'estabilidad', 'dropout', 'profundidad', 'ajustar', 'secuencias', 'tamaño', 'aumentar']"
Q044,M12T1,Generación de texto con RNN,¿Qué ventaja aporta usar celdas LSTM/GRU frente a una RNN simple para generación de texto?,Recuerda el problema del gradiente en secuencias largas.,"[""Mejor manejo de dependencias a largo plazo gracias a compuertas de memoria"", ""Mitigan el desvanecimiento/explosión del gradiente en secuencias largas"", ""Capturan contexto distante con mayor estabilidad que RNN simples""]","[""Eliminan por completo la necesidad de entrenamiento"", ""Permiten entrenar sin datos de texto etiquetado ni corpus"", ""Sustituyen la necesidad de tokenización y preprocesamiento""]","['memoria', 'estabilidad', 'secuencias', 'distante', 'gradiente', 'largas', 'mejor', 'contexto', 'dependencias', 'plazo']"
Q045,M12T2,Conjunto de datos y preprocesamiento (RNN/NLP),¿Cuál es el propósito de dividir el corpus en entrenamiento/validación/prueba en NLP?,Evalúa generalización y evita sobreajuste.,"[""Medir la capacidad de generalización del modelo en datos no vistos"", ""Evitar sobreajuste y calibrar hiperparámetros con validación"", ""Obtener una estimación honesta del rendimiento final con el conjunto de prueba""]","[""Aumentar artificialmente el tamaño del vocabulario"", ""Garantizar que todas las clases estén ausentes en validación"", ""Reutilizar la prueba para ajustar pesos en cada época""]","['validación', 'estimación', 'datos', 'rendimiento', 'prueba', 'generalización', 'modelo', 'capacidad', 'final', 'sobreajuste']"
Q046,M12T2,Conjunto de datos y preprocesamiento (RNN/NLP),Caso práctico: Tu dataset tiene fuerte desbalance de clases para análisis de sentimiento. ¿Qué estrategia inicial probarías?,Piensa en reponderación y re-muestreo.,"[""Aplicar re-muestreo (oversampling de la clase minoritaria)"", ""Usar pesos de clase en la función de pérdida"", ""Aumentar datos con técnicas de data augmentation textual controlado""]","[""Eliminar la clase minoritaria para simplificar el problema"", ""Mezclar entrenamiento con prueba para equilibrar"", ""Duplicar aleatoriamente la clase mayoritaria""]","['oversampling', 'datos', 'data', 'clase', 'minoritaria', 'augmentation', 'aumentar', 'pérdida', 'usar', 'técnicas']"
Q047,M12T2,Conjunto de datos y preprocesamiento (RNN/NLP),"¿Por qué es útil la tokenización consistente y la normalización (lowercasing, quitar signos) antes de entrenar?",Reduce variabilidad irrelevante.,"[""Disminuye la variación superficial y el ruido en el texto"", ""Reduce el tamaño del vocabulario sin perder significado clave"", ""Mejora la estabilidad y convergencia del entrenamiento""]","[""Incrementa necesariamente la precisión en imágenes"", ""Evita la necesidad de embeddings"", ""Permite entrenar sin GPU ni CPU""]","['vocabulario', 'ruido', 'entrenamiento', 'texto', 'disminuye', 'mejora', 'estabilidad', 'reduce', 'convergencia', 'variación']"
Q048,M12T3,Clasificación de opiniones / Análisis de sentimiento (RNN),"En análisis de sentimiento con RNN, ¿qué representa típicamente la salida final de la red?",Piensa en una probabilidad o etiqueta global.,"[""Una representación global del sentimiento del texto (probabilidad/etiqueta)"", ""Una probabilidad por clase que resume la secuencia completa"", ""La predicción de polaridad (positiva/negativa/neutral)""]","[""La transcripción fonética de cada palabra"", ""Las coordenadas de bounding boxes por token"", ""Un conjunto de imágenes con el sentimiento""]","['probabilidad', 'predicción', 'polaridad', 'negativa', 'positiva', 'neutral', 'clase', 'secuencia', 'global', 'sentimiento']"
Q049,M12T3,Clasificación de opiniones / Análisis de sentimiento (RNN),"Caso práctico: Si el modelo confunde sarcasmo con sentimiento positivo, ¿qué medida ayudaría?",Piensa en contexto y datos.,"[""Aumentar el dataset con ejemplos etiquetados de sarcasmo"", ""Incorporar modelos que capturen contexto más amplio (LSTM/GRU más profundas)"", ""Añadir características pragmáticas/contextuales o fine-tuning con datos específicos""]","[""Eliminar todas las oraciones irónicas del corpus de entrenamiento"", ""Cambiar a un modelo de visión por computadora"", ""Forzar que todas las reseñas largas sean negativas""]","['dataset', 'contextuales', 'sarcasmo', 'contexto', 'ejemplos', 'datos', 'lstm', 'etiquetados', 'características', 'modelos']"
Q050,M12T3,Clasificación de opiniones / Análisis de sentimiento (RNN),¿Qué ventaja tiene combinar modelado de temas (LDA/NMF) con análisis de sentimiento?,Relaciona temas con sentimiento por aspecto.,"[""Permite analizar sentimiento por cada tema o aspecto identificado"", ""Mejora la interpretación al vincular tópicos con polaridad"", ""Facilita priorizar acciones sobre temas con peor percepción""]","[""Evita la necesidad de etiquetar datos"", ""Convierte automáticamente texto en audio"", ""Sustituye el preprocesamiento de texto""]","['percepción', 'tópicos', 'analizar', 'interpretación', 'priorizar', 'polaridad', 'sentimiento', 'mejora', 'tema', 'temas']"
Q051,M12T4,Codificador–Decodificador y Traducción automática,¿Cuál es el rol del encoder en un modelo seq2seq para traducción?,Piensa en representar la entrada como un estado/vecino contextual.,"[""Codificar la secuencia de origen en una representación contextual compacta"", ""Capturar el significado global de la frase de entrada"", ""Proveer al decoder de una representación informativa para generar la salida""]","[""Generar directamente la frase traducida"", ""Calcular la pérdida de entrenamiento"", ""Reordenar las palabras de la salida final""]","['codificar', 'contextual', 'decoder', 'secuencia', 'origen', 'frase', 'generar', 'significado', 'informativa', 'representación']"
Q052,M12T4,Codificador–Decodificador y Traducción automática,Caso práctico: El decoder traduce mal palabras largas o compuestas. ¿Qué componente mejora este problema?,Atención ayuda a alinear.,"[""Agregar un mecanismo de atención para alinear mejor entrada y salida"", ""Usar atención global/local para enfocarse en segmentos relevantes"", ""Aplicar subword tokenization (BPE) para manejar palabras compuestas""]","[""Usar pooling máximo fijo en toda la entrada"", ""Eliminar el estado del encoder"", ""Forzar embeddings aleatorios en inferencia""]","['subword', 'tokenization', 'enfocarse', 'atención', 'usar', 'palabras', 'local', 'relevantes', 'agregar', 'segmentos']"
Q053,M12T4,Codificador–Decodificador y Traducción automática,¿Para qué sirve el 'teacher forcing' durante el entrenamiento del decoder?,Entrada del paso previo: ground truth vs. predicción.,"[""Acelerar el aprendizaje alimentando el token correcto anterior"", ""Reducir la acumulación de errores durante el entrenamiento"", ""Estabilizar la secuencia generada en fases iniciales""]","[""Obligar al modelo a memorizar todo el corpus"", ""Desactivar la retropropagación de gradientes"", ""Eliminar la necesidad de un conjunto de validación""]","['aprendizaje', 'entrenamiento', 'secuencia', 'estabilizar', 'errores', 'acelerar', 'acumulación', 'fases', 'reducir', 'alimentando']"
Q054,M12T5,Mecanismos de atención,¿Qué calcula el mecanismo de atención en modelos seq2seq?,Pesos sobre entradas relevantes.,"[""Pesos que indican qué partes de la entrada son más relevantes"", ""Una distribución (vía softmax) para ponderar contextos"", ""Un vector de contexto ponderado para el decoder""]","[""La tasa de aprendizaje óptima"", ""El número de épocas necesarias"", ""La longitud fija de la secuencia de salida""]","['contextos', 'contexto', 'softmax', 'decoder', 'relevantes', 'vector', 'partes', 'indican', 'entrada', 'ponderado']"
Q055,M12T5,Mecanismos de atención,Caso práctico: La atención parece 'difusa' y no se concentra en tokens clave. ¿Qué intentarías?,Ajusta arquitectura y regularización.,"[""Aumentar dimensionalidad de embeddings o cabezas de atención"", ""Ajustar temperatura/escala en dot-product attention"", ""Regularizar suavemente para evitar colapso o sobre-suavizado""]","[""Eliminar las codificaciones posicionales"", ""Deshabilitar normalización de capas"", ""Usar función de activación constante""]","['attention', 'dimensionalidad', 'escala', 'atención', 'ajustar', 'cabezas', 'temperatura', 'suavemente', 'aumentar', 'evitar']"
Q056,M12T5,Mecanismos de atención,¿Por qué la atención mejora la traducción respecto a un vector de contexto fijo?,Información localizada y dinámica.,"[""Permite acceder selectivamente a partes relevantes de la entrada en cada paso"", ""Evita comprimir toda la oración en un único vector"", ""Facilita alinear palabras y frases entre idiomas""]","[""Reduce el costo computacional en todos los casos"", ""Elimina la necesidad de embeddings"", ""Sustituye al encoder por completo""]","['idiomas', 'frases', 'selectivamente', 'vector', 'comprimir', 'palabras', 'evita', 'acceder', 'entrada', 'partes']"
Q057,M12T6,Modelos Transformers (NLP),¿Qué reemplaza la recurrencia en los Transformers para modelar dependencias?,Piensa en self-attention y paralelismo.,"[""El mecanismo de autoatención (self-attention)"", ""Atención multi-cabeza para capturar relaciones en paralelo"", ""Combinación de self-attention con codificaciones posicionales""]","[""Convoluciones 3D sobre la secuencia"", ""Pooling global sin pesos"", ""Celdas LSTM apiladas con compuertas""]","['autoatención', 'attention', 'self', 'atención', 'mecanismo', 'cabeza', 'paralelo', 'posicionales', 'relaciones', 'multi']"
Q058,M12T6,Modelos Transformers (NLP),Caso práctico: Tu BERT fine-tuned sobre-responde con frases largas irrelevantes. ¿Primer ajuste?,Regularización/early stopping/learning rate.,"[""Reducir la tasa de aprendizaje y aplicar early stopping"", ""Aumentar regularización (weight decay) y dropout moderado"", ""Rebalancear el dataset y revisar longitud máxima de secuencia""]","[""Eliminar las codificaciones posicionales para acortar respuestas"", ""Entrenar sin validación para ganar tiempo"", ""Desactivar el tokenizador y pasar texto crudo""]","['rebalancear', 'regularización', 'reducir', 'dataset', 'aprendizaje', 'dropout', 'decay', 'secuencia', 'aumentar', 'tasa']"
Q059,M12T6,Modelos Transformers (NLP),¿Para qué sirven las codificaciones posicionales en un Transformer?,Orden de la secuencia sin recurrencia.,"[""Introducir información de orden/posición en la secuencia"", ""Permitir que el modelo distinga tokens por su ubicación relativa"", ""Compensar la ausencia de recurrencia en la arquitectura""]","[""Reducir el tamaño del vocabulario"", ""Convertir texto en imagen"", ""Sustituir las capas feed-forward""]","['posición', 'secuencia', 'modelo', 'arquitectura', 'tokens', 'orden', 'recurrencia', 'relativa', 'ubicación', 'compensar']"
Q060,M12T7,Transformers para visión,¿Cómo procesa ViT una imagen de entrada?,Piensa en parches como tokens.,"[""Divide la imagen en parches y los trata como una secuencia de tokens"", ""Usa codificaciones posicionales para mantener información espacial"", ""Aplica self-attention sobre los embeddings de los parches""]","[""Convierte cada píxel en una palabra del diccionario"", ""Genera una máscara causal como en lenguaje"", ""Elimina toda información de posición""]","['parches', 'posicionales', 'tokens', 'codificaciones', 'embeddings', 'espacial', 'divide', 'imagen', 'secuencia', 'aplica']"
Q061,M12T7,Transformers para visión,"Caso práctico: En DETR, las predicciones de bounding boxes son imprecisas. ¿Qué revisarías primero?","Pérdida, anclas no se usan en DETR, revisa entrenamiento.","[""Ajustar la función de pérdida (combinando clasificación y regresión)"", ""Revisar el schedule de entrenamiento y la calidad de anotaciones"", ""Afinar el número de queries y el tamaño de la imagen de entrada""]","[""Añadir anclas manuales como en Faster R-CNN (DETR no usa anclas)"", ""Eliminar la pérdida de regresión de cajas"", ""Sustituir self-attention por pooling promedio""]","['clasificación', 'entrenamiento', 'regresión', 'pérdida', 'calidad', 'schedule', 'ajustar', 'revisar', 'queries', 'función']"
Q062,M12T7,Transformers para visión,¿Qué desafío habitual presentan los Transformers en visión frente a las CNN?,Datos y costo computacional.,"[""Requieren más datos y cómputo para alcanzar su máximo rendimiento"", ""Pueden sobreajustar con datasets pequeños si no se regularizan"", ""Suelen necesitar preentrenamiento extensivo""]","[""No pueden procesar imágenes en color"", ""No pueden entrenarse con GPUs"", ""Siempre tienen menos parámetros que una CNN""]","['datasets', 'datos', 'sobreajustar', 'rendimiento', 'preentrenamiento', 'extensivo', 'requieren', 'suelen', 'pueden', 'necesitar']"
Q063,M12T8,Transformers de Hugging Face,¿Qué ventaja ofrece usar `pipeline` de Hugging Face para una tarea NLP?,Abstracción de carga y pre/post-procesado.,"[""Facilita inferencia rápida integrando tokenización, modelo y postproceso"", ""Permite usar modelos preentrenados con pocas líneas de código"", ""Estandariza entradas y salidas para tareas comunes""]","[""Entrena automáticamente el modelo sin datos"", ""Elimina la necesidad de GPU"", ""Convierte modelos de visión en NLP""]","['postproceso', 'modelos', 'modelo', 'código', 'tokenización', 'tareas', 'integrando', 'inferencia', 'usar', 'preentrenados']"
Q064,M12T8,Transformers de Hugging Face,Caso práctico: El tokenizer corta términos técnicos clave. ¿Qué harías?,Tokens especiales o vocab ampliado.,"[""Añadir tokens al vocabulario y reentrenar/ajustar el tokenizer"", ""Usar subword tokenization adecuada (BPE/WordPiece)"", ""Realizar fine-tuning con corpus específico del dominio""]","[""Eliminar el tokenizer para pasar texto crudo"", ""Forzar longitud máxima extremadamente corta"", ""Desactivar codificaciones posicionales""]","['tokenization', 'tokenizer', 'tokens', 'subword', 'vocabulario', 'wordpiece', 'corpus', 'tuning', 'usar', 'ajustar']"
Q065,M12T8,Transformers de Hugging Face,¿Para qué sirve `from_pretrained` en Transformers?,Carga de pesos y configuración.,"[""Cargar un modelo y tokenizer con pesos preentrenados"", ""Recuperar configuración y vocabulario asociados al checkpoint"", ""Permitir fine-tuning sin entrenar desde cero""]","[""Compilar el modelo a lenguaje ensamblador"", ""Borrar el histórico de entrenamiento"", ""Crear un modelo sin arquitectura definida""]","['checkpoint', 'tuning', 'tokenizer', 'entrenar', 'preentrenados', 'cargar', 'configuración', 'pesos', 'cero', 'modelo']"
Q066,M12T9,Otras librerías de transformers (comparativa),"¿Qué criterio es clave al comparar librerías como Fairseq, OpenNMT o Tensor2Tensor?","Compatibilidad, comunidad, rendimiento.","[""Compatibilidad con frameworks y facilidad de integración"", ""Comunidad, documentación y ejemplos disponibles"", ""Rendimiento y soporte de entrenamiento distribuido""]","[""El número de colores del logo"", ""Que no tengan documentación para evitar sesgos"", ""Que obliguen a usar un solo backend""]","['frameworks', 'entrenamiento', 'compatibilidad', 'comunidad', 'soporte', 'distribuido', 'documentación', 'integración', 'ejemplos', 'disponibles']"
Q067,M12T9,Otras librerías de transformers (comparativa),Caso práctico: Necesitas inferencia ultrarrápida con modelos grandes. ¿Qué herramienta considerarías?,Optimización e inferencia: ONNX/DeepSpeed.,"[""Exportar a ONNX Runtime para acelerar la inferencia"", ""Usar DeepSpeed/quantización para optimizar memoria y latencia"", ""Aplicar compiladores/graph optimizers del framework""]","[""Aumentar el tamaño del batch indefinidamente"", ""Desactivar la vectorización para mayor control"", ""Entrenar desde cero sin preentrenamiento""]","['optimizers', 'runtime', 'deepspeed', 'optimizar', 'compiladores', 'quantización', 'graph', 'framework', 'onnx', 'acelerar']"
Q068,M12T9,Otras librerías de transformers (comparativa),¿Qué ventaja aporta un enfoque modular y configurable en estas librerías?,Reutilización y experimentación.,"[""Facilita experimentar con arquitecturas y cambios de componentes"", ""Permite reutilizar código entre tareas y proyectos"", ""Reduce tiempo de desarrollo y errores""]","[""Impide reproducibilidad de experimentos"", ""Obliga a usar únicamente RNNs"", ""Elimina la necesidad de pruebas""]","['proyectos', 'arquitecturas', 'reutilizar', 'tareas', 'componentes', 'código', 'reduce', 'desarrollo', 'cambios', 'facilita']"
Q069,M12T10,Aplicaciones prácticas NLP con RNN y atención,"En una app de NLP productiva, ¿qué debes monitorear tras el despliegue?",Métricas de rendimiento y de negocio.,"[""Métricas de modelo (precisión/AUC/latencia) y drifts de datos"", ""Disponibilidad, errores y tiempos de respuesta"", ""Indicadores de negocio/UX (CTR, satisfacción, quejas)""]","[""Solo el consumo de disco del servidor"", ""Únicamente la cantidad de epochs entrenadas"", ""La temperatura del color del dashboard""]","['indicadores', 'precisión', 'datos', 'latencia', 'errores', 'drifts', 'métricas', 'negocio', 'satisfacción', 'tiempos']"
Q070,M12T10,Aplicaciones prácticas NLP con RNN y atención,Caso práctico: El modelo degradó su rendimiento meses después del lanzamiento. ¿Posible causa y acción?,Data drift + reentrenamiento.,"[""Drift de datos del dominio; recolectar nuevos ejemplos y realizar reentrenamiento"", ""Monitorear distribución de features y recalibrar umbrales"", ""Implementar pipelines de actualización periódica con validación""]","[""Eliminar logs para ahorrar almacenamiento"", ""Aumentar el batch size de inferencia sin evaluar"", ""Deshabilitar el sistema de alertas para evitar notificaciones""]","['actualización', 'drift', 'datos', 'distribución', 'validación', 'features', 'ejemplos', 'recalibrar', 'monitorear', 'nuevos']"
Q071,M12T10,Aplicaciones prácticas NLP con RNN y atención,¿Qué pasos claves incluye llevar un prototipo de NLP a producción?,"Piensa en API, escalabilidad y pruebas.","[""Exponer el modelo vía API y agregar autenticación"", ""Asegurar escalabilidad, monitoreo y logging"", ""Pruebas A/B, integración y regresión antes de lanzar""]","[""Evitar pruebas con usuarios para ahorrar tiempo"", ""Publicar pesos del modelo sin control de acceso"", ""Quitar métricas para simplificar""]","['logging', 'autenticación', 'monitoreo', 'pruebas', 'integración', 'modelo', 'regresión', 'agregar', 'exponer', 'escalabilidad']"
